#!/usr/bin/env python

# Copyright (C) 2017 Duncan Brown and Derek Davis
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 3 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""Program for setting up a workflow which estimates the average PSD of a given
portion of strain data."""

import socket
import math
import pycbc
import pycbc.version
import pycbc.workflow
import os.path
import argparse
import logging
import pycbc_glue.segments
import datetime
import lal, sys
from pycbc.results import create_versioning_page, save_fig_with_metadata, two_column_layout, layout
from pycbc.workflow.core import Executable, File, FileList, Node, make_analysis_dir

class CrosstalkTransferFunctionExecutable(Executable):
    """Class for running pycbc_tf_measure
    """
    current_retention_level = Executable.INTERMEDIATE_PRODUCT

def create_tf_node(workflow, segment, aux_group, gate_file, 
                  ifo, hoft_data_cache, aux_data_cache, extra_tags=None):
    if extra_tags is None:
        extra_tags = []
    make_analysis_dir('tf_files')
    exe = CrosstalkTransferFunctionExecutable(
               workflow.cp, 'pycbc_tf_measure',
               ifos=ifo, out_dir='tf_files',
               tags=extra_tags)
    node = exe.create_node()
    node.add_opt('--gps-start-time', segment[0])
    node.add_opt('--gps-end-time', segment[1])

    chan = workflow.cp.get('aux_channels-'+aux_group,'aux-chan-group')
    lines = workflow.cp.get('aux_channels-'+aux_group,'lines')
    node.add_opt('--aux-chan-group', "'"+aux_group+','+chan+','+lines+"'") 

    duration = segment[1] - segment[0]-4
    node.add_opt('--tf-dur', duration)
    node.add_input_opt('--gate-file', gate_file)
    node.add_input_list_opt('--hoft-frame-loc', hoft_data_cache)
    node.add_input_list_opt('--aux-frame-loc', aux_data_cache)

    filter_length = workflow.cp.get('aux_channels-'+aux_group,'filter-length')
    if duration < int(filter_length):
        filter_length = duration
    node.add_opt('--filter-length', filter_length) 
    node.new_output_file_opt(segment, '.hdf', '--output-file',tags=extra_tags)
    workflow += node
    return node.output_files[0]

class CrosstalkMergeTransferFunctionExecutable(Executable):
    """Class for running pycbc_merge_tf
    """
    current_retention_level = Executable.MERGED_TRIGGERS

def create_merged_tf_node(workflow, segment, tf_file_list, ifo, extra_tags=None):
    if extra_tags is None:
        extra_tags = []
    make_analysis_dir('tf_files')
    exe = CrosstalkMergeTransferFunctionExecutable(
               workflow.cp, 'pycbc_merge_tf',
               ifos=ifo, out_dir='tf_files',
               tags=extra_tags)
    node = exe.create_node()
    node.add_input_list_opt('--hdf-tf-file', tf_file_list)
    node.new_output_file_opt(segment, '.hdf', '--output-hdf-file')
    workflow += node
    return node.output_files[0]

class CrosstalkProjectNoiseExecutable(Executable):
    """Class for running pycbc_project_noise_from_tf
    """
    current_retention_level = Executable.ALL_TRIGGERS

def create_project_noise_node(workflow, segment, tf_file, ifo, extra_tags=None):
    if extra_tags is None:
        extra_tags = []
    make_analysis_dir('noise_data')
    exe = CrosstalkProjectNoiseExecutable(
               workflow.cp, 'pycbc_project_noise_from_tf',
               ifos=ifo, out_dir='noise_data',
               tags=extra_tags)
    node = exe.create_node()
    node.add_opt('--gps-start-time', segment[0])
    node.add_opt('--gps-end-time', segment[1])
    node.add_input_opt('--tf-file', tf_file)
    node.new_output_file_opt(segment, '.gwf', '--output-frame-path', tags=extra_tags) 
    workflow += node
    return node.output_files[0]

class CrosstalkSubtractNoiseExecutable(Executable):
    """Class for running pycbc_subtract_noise
    """
    current_retention_level = Executable.FINAL_RESULT

def create_merged_frame_node(workflow, segment, noise_file_list, ifo, extra_tags=None):
    if extra_tags is None:
        extra_tags = []
    make_analysis_dir('output_frame_files')
    exe = CrosstalkSubtractNoiseExecutable(
               workflow.cp, 'pycbc_subtract_noise',
               ifos=ifo, out_dir='output_frame_files',
               tags=extra_tags)
    node = exe.create_node()
    node.add_input_list_opt('--noise-frame-file', noise_file_list) 
    store_file = node.executable.retain_files
    frame_type = workflow.cp.get('workflow-frames','output-frame-type') 
    fil = File(ifo[0], frame_type,
               segment, extension='.gwf', store_file=store_file, 
               directory=node.executable.out_dir, tags=None)
    node.add_output_opt('--output-frame-file', fil)
    workflow += node
    return node.output_files[0]

class PlotTransferFunctionExecutable(Executable):
    """Class for running pycbc_subtract_noise
    """
    current_retention_level = Executable.FINAL_RESULT

def create_plot_tf_node(workflow, channel, tf_file_list, outdir, ifo, extra_tags=None):
    if extra_tags is None:
        extra_tags = []
    exe = PlotTransferFunctionExecutable(
               workflow.cp, 'pycbc_plot_tf',
               ifos=ifo, out_dir=outdir,
               tags=extra_tags)
    node = exe.create_node()
    node.add_opt('--channel', channel)
    node.add_input_list_opt('--tf-file', tf_file_list)
    node.new_output_file_opt(workflow.analysis_time, '.png', '--outfile', tags=extra_tags)
    workflow += node
    return node.output_files[0]

def symlink_path(f, path):
    if f is None:
        return
    try:
        os.symlink(f.storage_path, os.path.join(path, f.name))
    except OSError:
        pass

def symlink_result(f, rdir_path):
    symlink_path(f, rdir[rdir_path])

logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s',
                    level=logging.INFO)

parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument('--version', action='version',
                    version=pycbc.version.git_verbose_msg)
parser.add_argument('--workflow-name', required=True)
parser.add_argument("-d", "--output-dir", required=True,
                    help="Path to output directory.")
pycbc.workflow.add_workflow_command_line_group(parser)
args = parser.parse_args()

pycbc.workflow.makedir(args.output_dir)

container = pycbc.workflow.Workflow(args, args.workflow_name)
workflow = pycbc.workflow.Workflow(args, args.workflow_name + '-main')
finalize_workflow = pycbc.workflow.Workflow(args, args.workflow_name + '-finalization')

os.chdir(args.output_dir)

rdir = layout.SectionNumber('results', ['analysis_time', 'detector_sensitivity', 
                                        'transfer_functions','workflow'])

pycbc.workflow.makedir(rdir.base)
pycbc.workflow.makedir(rdir['workflow'])

wf_log_file = pycbc.workflow.File(workflow.ifos, 'workflow-log', workflow.analysis_time,
                      extension='.txt',
                      directory=rdir['workflow'])

logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s',
                    filename=wf_log_file.storage_path,
                    level=logging.INFO,
                    filemode='w')

logfile = logging.FileHandler(filename=wf_log_file.storage_path,mode='w')
logfile.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s:%(levelname)s : %(message)s')
logfile.setFormatter(formatter)
logging.getLogger('').addHandler(logfile)
logging.info("Created log file %s" % wf_log_file.storage_path)


# put start / end time at top of summary page
time = workflow.analysis_time
s, e = int(time[0]), int(time[1])
s_utc = str(datetime.datetime(*lal.GPSToUTC(s)[0:6]))
e_utc = str(datetime.datetime(*lal.GPSToUTC(e)[0:6]))
time_str = '<center><p><b>GPS Interval [%s,%s). UTC Interval %s - %s. Interval duration = %.3f days.</b></p></center>' % (s, e, s_utc, e_utc, float(e-s)/86400.0)
time_file = pycbc.workflow.File(workflow.ifos, 'time', workflow.analysis_time,
                                           extension='.html',
                                           directory=rdir.base)

kwds = { 'title' : 'Search Workflow Duration (Wall Clock Time)', 
        'caption' : "Wall clock start and end times for this invocation of the workflow. "
                    " The command line button shows the arguments used to invoke the workflow "
                    " creation script.",
        'cmd' :' '.join(sys.argv), }
save_fig_with_metadata(time_str, time_file.storage_path, **kwds)

# Get segments and find where the data is
seg_dir = rdir['analysis_time/segment_data']
pycbc.workflow.makedir(seg_dir)

veto_cat_files = pycbc.workflow.get_files_for_vetoes(
        workflow, seg_dir, runtime_names=['segments-science-veto'])

science_seg_file, science_segs, sci_seg_name = \
        pycbc.workflow.get_science_segments(workflow, seg_dir)

sci_ok_seg_file, science_ok_segs, sci_ok_seg_name = \
        pycbc.workflow.get_analyzable_segments(workflow, science_segs,
                                                veto_cat_files, seg_dir)

datafind_files, analyzable_file, analyzable_segs, analyzable_name = \
        pycbc.workflow.setup_datafind_workflow(workflow, science_ok_segs,
                                               'datafind',
                                               seg_file=science_seg_file, 
                                               tags = ['hoft'])

datafind_files_aux, _, _, _ = \
        pycbc.workflow.setup_datafind_workflow(workflow, science_ok_segs,
                                               'datafind',
                                               seg_file=science_seg_file, 
                                               tags = ['aux'])

# get gating file
gate_files = pycbc.workflow.setup_gating_workflow(workflow, output_dir="gating")

# make veto definer table
vetodef_table = pycbc.workflow.make_veto_table(workflow, rdir['analysis_time/veto_definer'])
layout.single_layout(rdir['analysis_time/veto_definer'], ([vetodef_table]))

# ================================================================================

# Set up noise removal workflow

sing_ifo = workflow.ifos[0]

finalized_frame_files = []
finalized_frame_segments = {}
frame_psd_segments = {}
merged_tf_files = {}
psd_files = []
data_analysed_name = 'FRAME_PRODUCED'

for ifo, segments in analyzable_segs.items():
    for seg in segments:
        frame_files = {}
         
        frame_duration = int(workflow.cp.get('pycbc_project_noise_from_tf','file-size'))

        for aux_group in workflow.cp.get_subsections('aux_channels'):
            if workflow.cp.has_option_tags('valid_times-'+aux_group, 'gps-start-time', tags=None):
                valid_start = int(workflow.cp.get_opt_tags('valid_times-'+aux_group, 
                                                 'gps-start-time', tags=None))
                valid_end = int(workflow.cp.get_opt_tags('valid_times-'+aux_group,
                                                 'gps-end-time', tags=None))
                valid_segment = pycbc_glue.segments.segment(valid_start, valid_end)
                if (seg in valid_segment) == False: 
                    logging.warning('Aux group {} not valid during this segment'.format(aux_group))
                    continue
            logging.info('Working on aux group {}'.format(aux_group))
            duration_tf = int(workflow.cp.get('aux_channels-'+aux_group,'duration-tf'))
            duration_data = seg[1] - seg[0] - 4
            #Set up tf start times
            if duration_data < duration_tf:
                #largest power of 2 < duration
                duration_tf = 2 ** int(math.log(duration_data, 2))
                logging.info('TF duration is too long, new duration is {}'.format(duration_tf))
            tf_start_list = range(seg[0]+2, seg[1]-2, duration_tf/2)
            i = 0
            while i < len(tf_start_list):
                if (tf_start_list[i]+duration_tf) > seg[1]:
                    tf_start_list.pop(i)
                else:
                    i+=1

            # Set up frame times
            frame_start_list = range(seg[0]+2, seg[1], frame_duration)
        
            tf_files = []

            for start_time in tf_start_list:
                segment_dur = pycbc_glue.segments.segment(start_time-2, start_time + duration_tf+2)
                hdf_tf_file = create_tf_node(workflow, segment_dur, aux_group, gate_files[0], sing_ifo, 
                                             datafind_files, datafind_files_aux, extra_tags=[aux_group])
                tf_files.append(hdf_tf_file)

            #Now run the merge job
            merged_hdf_tf_file = create_merged_tf_node(workflow, seg, tf_files, sing_ifo, extra_tags=[aux_group])
            logging.info('Created transfer function file for {} - {}'.format(seg[0],seg[1]))
            if aux_group not in merged_tf_files:
                merged_tf_files[aux_group] = []
            merged_tf_files[aux_group].append(merged_hdf_tf_file)

            #Take that hdf file and use it as the input to the frame write job
            for start_time in frame_start_list:
                end_time = min(start_time+frame_duration,seg[1])
                frame_segment = pycbc_glue.segments.segment(start_time, end_time)
                frame_file = create_project_noise_node(workflow, frame_segment, merged_hdf_tf_file, sing_ifo, extra_tags=[aux_group])
                if start_time not in frame_files:
                    frame_files[start_time] = []
                frame_files[start_time].append(frame_file)

        for frame_time in frame_files:
            end_time = min(frame_time+frame_duration,seg[1])
            frame_segment = pycbc_glue.segments.segment(frame_time, end_time)
            merged_frame_file = create_merged_frame_node(workflow, frame_segment, frame_files[frame_time], sing_ifo, extra_tags=None)
            finalized_frame_files.append(merged_frame_file)
            if ifo not in finalized_frame_segments:
                finalized_frame_segments[ifo] = pycbc_glue.segments.segmentlist()
            finalized_frame_segments[ifo] |= pycbc_glue.segments.segmentlist([frame_segment])
            logging.info('Created frame file for {} - {}'.format(frame_segment[0],frame_segment[1]))

            #set up segments to calculate psd over
            if ifo not in frame_psd_segments:
                frame_psd_segments[ifo] = pycbc_glue.segments.segmentlist()
            psd_segs = range(frame_time, end_time, 2048)
            psd_segs.append(end_time)
            for item in range(0,len(psd_segs)-1):
                seg_to_add = pycbc_glue.segments.segment(psd_segs[item],psd_segs[item+1]-1)
                frame_psd_segments[ifo] |= pycbc_glue.segments.segmentlist([seg_to_add])
       
    long_frame_segments = [x for x in frame_psd_segments[ifo] if abs(x) > 700]
    psd_files += [pycbc.workflow.setup_psd_calculate(workflow, finalized_frame_files, ifo,
              long_frame_segments, data_analysed_name, 'psds')]


    

# ================================================================================

# Now set up results page jobs with finalized files

# set up strain spectrum and range plots
logging.info('Setting up summary plots')
pycbc.workflow.makedir(rdir['detector_sensitivity'])
spec_plot = pycbc.workflow.make_spectrum_plot(workflow, psd_files, rdir['detector_sensitivity'])
range_plot_summ = pycbc.workflow.make_range_plot(workflow, psd_files, rdir['detector_sensitivity'],
                       require='summ')
range_plot_other = pycbc.workflow.make_range_plot(workflow, psd_files, rdir['detector_sensitivity'],
                        exclude='summ')

det_summ = [(spec_plot, range_plot_summ[0] if len(range_plot_summ) != 0 else None)]
layout.two_column_layout(rdir['detector_sensitivity'],
                     det_summ + list(layout.grouper(range_plot_other, 2)))

############################## Setup the transfer function plots #######################

#set up tranfer function jobs
for aux_group in workflow.cp.get_subsections('aux_channels'):
    if aux_group not in merged_tf_files:
        logging.warning('Aux group {} has no transfer functions to plot'.format(aux_group))
        continue
    tf_plot_list = []
    tf_dir = rdir['transfer_functions/%s'%aux_group]
    pycbc.workflow.makedir(tf_dir)
    chan = workflow.cp.get('aux_channels-'+aux_group,'aux-chan-group')
    channel_list = chan.split(',')
    for channel in channel_list:
        tf_plot = create_plot_tf_node(workflow, channel, merged_tf_files[aux_group], 
                  tf_dir, sing_ifo, extra_tags=[aux_group,channel[3:]])
        tf_plot_list += [(tf_plot,)]
        two_column_layout(tf_dir,tf_plot_list)

############################## Setup the segment information #######################

# get data segments to write to segment summary XML file
seg_summ_names    = ['DATA', 'SCIENCE_OK', 'ANALYZABLE_DATA', 'FRAMES_PRODUCED']
seg_summ_seglists = [science_segs, science_ok_segs, analyzable_segs, finalized_frame_segments]

# write segment summary XML file
seg_list = []
names = []
ifos = []
seg_plots = []
for segment_list,segment_name in zip(seg_summ_seglists, seg_summ_names):
    for ifo in workflow.ifos:
        seg_list.append(segment_list[ifo])
        names.append(segment_name)
        ifos.append(ifo)
seg_summ_file = pycbc.workflow.SegFile.from_multi_segment_list(
                   'WORKFLOW_SEGMENT_SUMMARY', seg_list, names, ifos,
                   valid_segment=workflow.analysis_time, extension='xml',
                   directory=seg_dir)
seg_sci_plot = pycbc.workflow.make_segments_plot(workflow,
                                  pycbc.workflow.FileList([science_seg_file]),
                                  rdir['analysis_time'], tags=['SCIENCE_MINUS_CAT1'])

seg_plots += [(seg_sci_plot,)]

# make segment table for summary page
seg_summ_table = pycbc.workflow.make_seg_table(workflow, [seg_summ_file],
        seg_summ_names, rdir['analysis_time'], ['SUMMARY'],
        title_text='Input and output time',
        description='This shows the total amount of input data, analyzable data, and the time for which transfer functions are produced.')
seg_plots += [(seg_summ_table,)]

curr_files = [science_seg_file, sci_ok_seg_file, analyzable_file]
curr_names = [sci_seg_name, sci_ok_seg_name, analyzable_name]
seg_summ_plot = pycbc.workflow.make_seg_plot(workflow, curr_files,
                        rdir['analysis_time'],
                        curr_names, ['SUMMARY'])
seg_plots += [(seg_summ_plot,)]

two_column_layout(rdir['analysis_time'], seg_plots)

# make front summary page
summ = ([(time_file,)] + [(seg_summ_plot,)] +
        [(seg_summ_table, )] + det_summ)
for row in summ:
    for f in row:
        symlink_path(f, rdir.base)
layout.two_column_layout(rdir.base, summ)

# save global config file to results directory
base = rdir['workflow/configuration']
pycbc.workflow.makedir(base)
ini_file_path = os.path.join(base, 'configuration.ini')
with open(ini_file_path, 'wb') as ini_fh:
    container.cp.write(ini_fh)
ini_file = pycbc.workflow.FileList([pycbc.workflow.File(workflow.ifos, '', workflow.analysis_time,
                        file_url='file://'+ini_file_path)])
layout.single_layout(base, ini_file)

# Create versioning information
create_versioning_page(rdir['workflow/version'], container.cp)

# Create the final log file
log_file_html = pycbc.workflow.File(workflow.ifos, 'WORKFLOW-LOG', workflow.analysis_time,
                                           extension='.html',
                                           directory=rdir['workflow'])

# Create a page to contain a dashboard link
dashboard_file = pycbc.workflow.File(workflow.ifos, 'DASHBOARD', workflow.analysis_time,
                                           extension='.html',
                                           directory=rdir['workflow'])
dashboard_str = """<center><p style="font-size:20px"><b><a href="PEGASUS_DASHBOARD_URL" target="_blank">Pegasus Dashboard Page</a></b></p></center>"""
kwds = { 'title' : 'Pegasus Dashboard',
         'caption' : "Link to Pegasus Dashboard",
         'cmd' : "PYCBC_SUBMIT_DAX_ARGV", }
save_fig_with_metadata(dashboard_str, dashboard_file.storage_path, **kwds)

# Create pages for the submission script to write data
pycbc.workflow.makedir(rdir['workflow/dax'])
pycbc.workflow.makedir(rdir['workflow/input_map'])
pycbc.workflow.makedir(rdir['workflow/output_map'])
pycbc.workflow.makedir(rdir['workflow/planning'])

pycbc.workflow.make_results_web_page(finalize_workflow,
                                     os.path.join(os.getcwd(),
                                     rdir.base))

container += workflow
container += finalize_workflow

import Pegasus.DAX3 as dax
dep = dax.Dependency(parent=workflow.as_job, child=finalize_workflow.as_job)
container._adag.addDependency(dep)

container.save()

logging.info("Written dax.")

# Close the log and flush to the html file
logging.shutdown()
with open (wf_log_file.storage_path, "r") as logfile:
    logdata=logfile.read()
log_str = """
<p>Workflow generation script created workflow in output directory: %s</p>
<p>Workflow name is: %s</p>
<p>Workflow generation script run on host: %s</p>
<pre>%s</pre>
""" % (os.getcwd(), args.workflow_name, socket.gethostname(), logdata)
kwds = { 'title' : 'Workflow Generation Log',
         'caption' : "Log of the workflow script %s" % sys.argv[0],
         'cmd' :' '.join(sys.argv), }
save_fig_with_metadata(log_str, log_file_html.storage_path, **kwds)
layout.single_layout(rdir['workflow'], ([dashboard_file,log_file_html]))

